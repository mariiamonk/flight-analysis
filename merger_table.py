# -*- coding: utf-8 -*-
import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler

# ==================================================
# 0. –ó–ê–ì–†–£–ó–ö–ê
# ==================================================
df = pd.read_csv(r"/home/mariia/–ó–∞–≥—Ä—É–∑–∫–∏/Telegram Desktop/AI2/data_staging/merged_all_detailed.csv", low_memory=False)
print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df):,} —Å—Ç—Ä–æ–∫, {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫")

# –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–∏–ø–æ–≤
df["flight_date"] = pd.to_datetime(df["flight_date"], errors="coerce")

# ==================================================
# 1. –ì–†–£–ü–ü–ò–†–û–í–ö–ê –ü–û –ß–ï–õ–û–í–ï–ö–£ (–∞ –Ω–µ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É)
# ==================================================
grouped = df.groupby(["first_name", "last_name", "pax_birth_data"])

# --- –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ ---
agg = grouped.agg(
    n_flights_total=("flight_code", "count"),
    n_unique_documents=("document_norm", "nunique"),
    n_unique_routes=("flight_code", lambda x: len(set(zip(df.loc[x.index, "departure"], df.loc[x.index, "arrival"])))),
    n_unique_agents=("agent_info", "nunique"),
    baggage_ratio=("baggage", lambda x: (x != "").mean())
).reset_index()

# --- –í—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã –º–µ–∂–¥—É —Ä–µ–π—Å–∞–º–∏ ---
df_sorted = df.sort_values(["first_name", "last_name", "pax_birth_data", "flight_date"])
df_sorted["gap_days"] = df_sorted.groupby(["first_name", "last_name", "pax_birth_data"])["flight_date"].diff().dt.days

gap_stats = (
    df_sorted.groupby(["first_name", "last_name", "pax_birth_data"])["gap_days"]
    .agg(mean_time_between_flights="mean", min_time_between_flights="min")
    .reset_index()
)

# –ü—Ä–∏—Å–æ–µ–¥–∏–Ω—è–µ–º –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã –æ–±—Ä–∞—Ç–Ω–æ –∫ agg
agg = agg.merge(gap_stats, on=["first_name", "last_name", "pax_birth_data"], how="left")

# --- –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤–æ –≤—Ä–µ–º–µ–Ω–∏ ---
agg["days_active"] = grouped["flight_date"].apply(
    lambda x: (x.max() - x.min()).days if len(x.dropna()) > 1 else 0
).values
agg["flights_per_month"] = agg["n_flights_total"] / ((agg["days_active"] / 30).replace(0, 1))

# --- –î–æ–ª—è –ø—Ä–æ–ø—É—Å–∫–æ–≤ –ø–æ –∫–ª—é—á–µ–≤—ã–º –ø–æ–ª—è–º ---
def calc_missing_ratio(subdf):
    key_fields = ["fare", "baggage", "agent_info"]
    existing = [f for f in key_fields if f in subdf.columns]
    if not existing:
        return np.nan
    return (subdf[existing] == "").mean().mean()

agg["missing_ratio"] = grouped.apply(calc_missing_ratio).values

# --- –ß–∞—Å—Ç–æ—Ç–∞ –º–∞—Ä—à—Ä—É—Ç–æ–≤ –∏ –∞–≥–µ–Ω—Ç–æ–≤ ---
route_counts = df.groupby(["departure", "arrival"]).size().rename("route_freq")
df = df.merge(route_counts, on=["departure", "arrival"], how="left")

agent_counts = df["agent_info"].value_counts().to_dict()
agg["avg_route_popularity"] = grouped["flight_code"].apply(
    lambda x: np.mean(df.loc[x.index, "route_freq"])
).values
agg["avg_agent_popularity"] = grouped["agent_info"].apply(
    lambda x: np.mean([agent_counts.get(a, 0) for a in x])
).values

# ==================================================
# 2. –ú–û–î–ï–õ–¨ Isolation Forest
# ==================================================
features = agg.drop(columns=["first_name", "last_name", "pax_birth_data"]).fillna(0)

# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

# Isolation Forest
model = IsolationForest(
    n_estimators=300,
    contamination=0.02,  # –æ–∂–∏–¥–∞–µ–º 2% –∞–Ω–æ–º–∞–ª–∏–π
    random_state=42
)
model.fit(features_scaled)

agg["anomaly_score"] = model.decision_function(features_scaled)
agg["is_suspicious"] = model.predict(features_scaled)  # -1 = –∞–Ω–æ–º–∞–ª–∏—è

# ==================================================
# 3. –ü–û–Ø–°–ù–ï–ù–ò–ï ‚Äú–ü–û–ß–ï–ú–£ –ü–û–î–û–ó–†–ò–¢–ï–õ–ï–ù‚Äù
# ==================================================
def explain(row):
    reasons = []
    if row["n_unique_documents"] > 1:
        reasons.append(f"–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª {int(row['n_unique_documents'])} —Ä–∞–∑–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    if row["flights_per_month"] > 10:
        reasons.append("—á—Ä–µ–∑–º–µ—Ä–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ –ø–µ—Ä–µ–ª—ë—Ç–æ–≤")
    if row["baggage_ratio"] < 0.2 and row["n_flights_total"] > 5:
        reasons.append("—á–∞—Å—Ç—ã–µ —Ä–µ–π—Å—ã –±–µ–∑ –±–∞–≥–∞–∂–∞")
    if row["missing_ratio"] > 0.3:
        reasons.append("–º–Ω–æ–≥–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö")
    if row["n_unique_agents"] > 5:
        reasons.append("–∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç—Å—Ç–≤ –ø—Ä–æ–¥–∞–∂")
    if not reasons:
        reasons.append("–∞–Ω–æ–º–∞–ª–∏—è –ø–æ —Å–æ–≤–æ–∫—É–ø–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    return "; ".join(reasons)

agg["reason"] = agg.apply(explain, axis=1)

# ==================================================
# 4. –í–´–í–û–î –ò –°–û–•–†–ê–ù–ï–ù–ò–ï
# ==================================================
suspects = agg[agg["is_suspicious"] == -1].sort_values("anomaly_score")

print(f"\nüö® –ù–∞–π–¥–µ–Ω–æ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –ª–∏—á–Ω–æ—Å—Ç–µ–π: {len(suspects)} / {len(agg)}")
print(suspects[[
    "first_name", "last_name", "pax_birth_data",
    "n_unique_documents", "n_flights_total", "flights_per_month",
    "n_unique_agents", "baggage_ratio",
    "missing_ratio", "reason", "anomaly_score"
]].head(15).to_string(index=False))

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á—ë—Ç–∞
out_path = r"/home/mariia/–ó–∞–≥—Ä—É–∑–∫–∏/Telegram Desktop/AI2/suspicious_passengers_detailed.csv"
agg.to_csv(out_path, index=False, encoding="utf-8")
print(f"\nüíæ –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω ‚Üí {out_path}")
